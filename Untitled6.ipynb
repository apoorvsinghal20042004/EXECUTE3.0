{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorvsinghal20042004/EXECUTE3.0/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESLBrneRLDqu",
        "outputId": "1282541b-91ad-4b91-c070-d201f01018c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.1.4)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n",
            "Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import pickle\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('sales.csv')\n",
        "\n",
        "# Convert 'Date' to datetime format\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "# Extract features from 'Date'\n",
        "data['Day'] = data['Date'].dt.day\n",
        "data['WeekOfYear'] = data['Date'].dt.isocalendar().week\n",
        "data['Month'] = data['Date'].dt.month\n",
        "data['Year'] = data['Date'].dt.year\n",
        "\n",
        "# Create Lag Features\n",
        "data['SalesVolume_Lag1'] = data['SalesVolume'].shift(1).fillna(0)\n",
        "data['Revenue_Lag1'] = data['Revenue'].shift(1).fillna(0)\n",
        "\n",
        "# Drop 'Date' column if not needed\n",
        "data.drop(['Date'], axis=1, inplace=True)\n",
        "\n",
        "# Convert 'CustomerIncome' to numeric\n",
        "data['CustomerIncome'] = data['CustomerIncome'].replace({'Low': 1, 'Medium': 2, 'High': 3}).astype(int)\n",
        "\n",
        "# Convert categorical columns to category type\n",
        "categorical_columns = ['DayOfWeek', 'Season', 'Event', 'ProductID', 'ProductCategory',\n",
        "                       'CustomerGender', 'CustomerLocation', 'CustomerSegment']\n",
        "\n",
        "for col in categorical_columns:\n",
        "    data[col] = data[col].astype('category')\n",
        "\n",
        "# One-Hot Encoding for Categorical Variables\n",
        "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Log Transformation of skewed features\n",
        "data['Revenue'] = np.log1p(data['Revenue'])\n",
        "data['MarketingSpend'] = np.log1p(data['MarketingSpend'])\n",
        "data['SalesVolume'] = np.log1p(data['SalesVolume'])\n",
        "\n",
        "# Define features and target variables\n",
        "X = data.drop(['SalesVolume', 'Revenue'], axis=1)\n",
        "y_sales = data['SalesVolume']\n",
        "y_revenue = data['Revenue']\n",
        "\n",
        "# Save the full feature names before feature selection\n",
        "full_feature_names = X.columns.tolist()\n",
        "\n",
        "# Feature Selection using Feature Importance from RandomForest (or any model)\n",
        "feature_selector = SelectFromModel(xgb.XGBRegressor(n_estimators=100, random_state=42), threshold='median')\n",
        "feature_selector.fit(X, y_sales)\n",
        "X_selected = feature_selector.transform(X)\n",
        "\n",
        "# Get selected feature names based on the feature selector\n",
        "selected_features = np.array(full_feature_names)[feature_selector.get_support()].tolist()\n",
        "\n",
        "# Convert selected features back to a DataFrame with the original feature names\n",
        "X_selected = pd.DataFrame(X_selected, columns=selected_features)\n",
        "\n",
        "# Split data for training and testing\n",
        "X_train_sales, X_test_sales, y_train_sales, y_test_sales = train_test_split(X_selected, y_sales, test_size=0.2, random_state=42)\n",
        "X_train_revenue, X_test_revenue, y_train_revenue, y_test_revenue = train_test_split(X_selected, y_revenue, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save the selected feature names to be used during prediction\n",
        "with open('selected_features.pkl', 'wb') as f:\n",
        "    pickle.dump(selected_features, f)\n",
        "\n",
        "# Define XGBoost model with reduced parameters\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5, subsample=0.8, colsample_bytree=0.8)\n",
        "\n",
        "# Use RandomizedSearchCV for Hyperparameter Tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.6, 0.8],\n",
        "    'colsample_bytree': [0.6, 0.8]\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning for Sales Volume model\n",
        "random_search_sales = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, scoring='neg_mean_squared_error', cv=TimeSeriesSplit(n_splits=3), verbose=1, n_jobs=-1, n_iter=10)\n",
        "random_search_sales.fit(X_train_sales, y_train_sales)\n",
        "\n",
        "# Use the best estimator for Sales Volume\n",
        "best_xgb_sales_model = random_search_sales.best_estimator_\n",
        "best_xgb_sales_model.fit(X_train_sales, y_train_sales)\n",
        "\n",
        "# Hyperparameter tuning for Revenue model\n",
        "random_search_revenue = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, scoring='neg_mean_squared_error', cv=TimeSeriesSplit(n_splits=3), verbose=1, n_jobs=-1, n_iter=10)\n",
        "random_search_revenue.fit(X_train_revenue, y_train_revenue)\n",
        "\n",
        "# Use the best estimator for Revenue\n",
        "best_xgb_revenue_model = random_search_revenue.best_estimator_\n",
        "best_xgb_revenue_model.fit(X_train_revenue, y_train_revenue)\n",
        "\n",
        "# Evaluate models\n",
        "sales_predictions = best_xgb_sales_model.predict(X_test_sales)\n",
        "revenue_predictions = best_xgb_revenue_model.predict(X_test_revenue)\n",
        "print(f\"Sales Volume MSE: {mean_squared_error(y_test_sales, sales_predictions)}\")\n",
        "print(f\"Revenue MSE: {mean_squared_error(y_test_revenue, revenue_predictions)}\")\n",
        "\n",
        "# Save the improved models as pickle files\n",
        "with open('optimized_sales_volume_model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_xgb_sales_model, f)\n",
        "\n",
        "with open('optimized_revenue_model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_xgb_revenue_model, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmQzLtwQMb89",
        "outputId": "0210b031-3ba2-4ec4-8f5f-c55ec5d50589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Sales Volume MSE: 0.019403468206711304\n",
            "Revenue MSE: 0.018395438150712918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K-QDIaZ5MeD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gwumpQdpZETw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}